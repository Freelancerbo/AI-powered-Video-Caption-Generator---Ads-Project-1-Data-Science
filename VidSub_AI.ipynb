{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsBmMlIrXjYx",
        "outputId": "16d30ec9-8115-49c1-fb41-083fbe574945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.6/803.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=ce0219c7c0cfd2814901ce27736760f1db5dc5e8c3c9561ee87837d52f0868e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: ffmpeg-python, openai-whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 openai-whisper-20250625\n"
          ]
        }
      ],
      "source": [
        "pip install openai-whisper ffmpeg-python opencv-python numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import ffmpeg\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# ---------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------\n",
        "VIDEO_PATH = \"input_video.mp4\"        # 👈 Replace with your video path\n",
        "OUTPUT_PATH = \"output_with_subtitles.mp4\"\n",
        "TARGET_LANGUAGE = \"hi\"                # \"en\"=English, \"hi\"=Hindi, \"ur\"=Urdu, etc.\n",
        "WHISPER_MODEL = \"base\"                # \"small\" / \"medium\" = more accurate but slower\n",
        "\n",
        "# ---------------------\n",
        "# STEP 1: VIDEO SE AUDIO EXTRACT KARO\n",
        "# ---------------------\n",
        "print(\"🔄 Extracting audio from video...\")\n",
        "with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_audio:\n",
        "    audio_path = tmp_audio.name\n",
        "\n",
        "try:\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(VIDEO_PATH)\n",
        "        .output(audio_path, acodec=\"pcm_s16le\", ac=1, ar=\"16000\")\n",
        "        .overwrite_output()\n",
        "        .run(capture_stdout=True, capture_stderr=True)\n",
        "    )\n",
        "    print(\"✅ Audio extracted successfully!\")\n",
        "except ffmpeg.Error as e:\n",
        "    print(\"❌ Error extracting audio:\", e.stderr.decode())\n",
        "    if os.path.exists(audio_path):\n",
        "        os.unlink(audio_path)\n",
        "    exit()\n",
        "\n",
        "# ---------------------\n",
        "# STEP 2: WHISPER SE SPEECH TO TEXT\n",
        "# ---------------------\n",
        "print(f\"🎙️ Loading Whisper model ({WHISPER_MODEL})...\")\n",
        "model = whisper.load_model(WHISPER_MODEL)\n",
        "\n",
        "print(\"🗣️ Transcribing audio to text...\")\n",
        "# If you want auto-language detection → remove `language=TARGET_LANGUAGE`\n",
        "result = model.transcribe(audio_path, language=TARGET_LANGUAGE, verbose=True)\n",
        "segments = result[\"segments\"]\n",
        "\n",
        "# ---------------------\n",
        "# STEP 3: VIDEO PAR SUBTITLES OVERLAY KARO\n",
        "# ---------------------\n",
        "print(\"🎬 Processing video with subtitles...\")\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)  # keep float for accuracy\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Output video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "# Font settings\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 0.7\n",
        "color = (255, 255, 255)\n",
        "thickness = 2\n",
        "line_type = cv2.LINE_AA\n",
        "\n",
        "def get_current_text(current_time):\n",
        "    for seg in segments:\n",
        "        if seg[\"start\"] <= current_time < seg[\"end\"]:\n",
        "            return seg[\"text\"]\n",
        "    return \"\"\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_time = frame_count / fps\n",
        "    subtitle_text = get_current_text(current_time)\n",
        "\n",
        "    if subtitle_text:\n",
        "        max_width = width - 100\n",
        "        words = subtitle_text.split()\n",
        "        lines, current_line = [], \"\"\n",
        "\n",
        "        for word in words:\n",
        "            test_line = f\"{current_line} {word}\".strip()\n",
        "            (w, _), _ = cv2.getTextSize(test_line, font, font_scale, thickness)\n",
        "            if w <= max_width:\n",
        "                current_line = test_line\n",
        "            else:\n",
        "                lines.append(current_line)\n",
        "                current_line = word\n",
        "        lines.append(current_line)\n",
        "\n",
        "        y_offset = height - 80\n",
        "        for line in lines:\n",
        "            (w, h), _ = cv2.getTextSize(line, font, font_scale, thickness)\n",
        "            x = (width - w) // 2\n",
        "            cv2.putText(frame, line, (x, y_offset), font, font_scale, color, thickness, line_type)\n",
        "            y_offset += int(h * 1.5)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "    print(f\"⏳ Frame {frame_count} processed...\", end=\"\\r\")\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"\\n🎉 Done! Subtitled video saved as: {OUTPUT_PATH}\")\n",
        "\n",
        "# Clean up temp audio\n",
        "if os.path.exists(audio_path):\n",
        "    os.unlink(audio_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "poPrykFUaazA",
        "outputId": "f06194a8-48e3-4672-8145-60371f1b5759"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Extracting audio from video...\n",
            "❌ Error extracting audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "input_video.mp4: No such file or directory\n",
            "\n",
            "🎙️ Loading Whisper model (base)...\n",
            "🗣️ Transcribing audio to text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\n/tmp/tmpmdfpal_m.wav: No such file or directory\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    572\u001b[0m                                      output=stdout, stderr=stderr)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', '/tmp/tmpmdfpal_m.wav', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 1.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1127806828.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🗣️ Transcribing audio to text...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# If you want auto-language detection → remove `language=TARGET_LANGUAGE`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_LANGUAGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"segments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Pad 30-seconds of silence to the input audio, for slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mcontent_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mN_FRAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mcontent_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_frames\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHOP_LENGTH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m32768.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\n/tmp/tmpmdfpal_m.wav: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "import sys\n",
        "\n",
        "# ---------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------\n",
        "VIDEO_PATH = \"input_video.mp4\"        # 👈 YEH FILE HONA CHAHIYE!\n",
        "OUTPUT_PATH = \"output_with_subtitles.mp4\"\n",
        "TARGET_LANGUAGE = \"hi\"\n",
        "WHISPER_MODEL = \"base\"\n",
        "\n",
        "# ---------------------\n",
        "# STEP 0: CHECK IF VIDEO EXISTS\n",
        "# ---------------------\n",
        "if not os.path.exists(VIDEO_PATH):\n",
        "    print(f\"❌ CRITICAL ERROR: Video file '{VIDEO_PATH}' not found!\")\n",
        "    print(\"   Please place your video file in the same folder as this script.\")\n",
        "    print(\"   Rename it to 'input_video.mp4' OR update VIDEO_PATH variable.\")\n",
        "    print(\"\\n   Example:\")\n",
        "    print(\"      - Your video: 'my_cat_video.mp4'\")\n",
        "    print(\"      - Rename it to: 'input_video.mp4'\")\n",
        "    print(\"      - Or change line: VIDEO_PATH = 'my_cat_video.mp4'\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"✅ Found video: {VIDEO_PATH}\")\n",
        "\n",
        "# ---------------------\n",
        "# STEP 1: EXTRACT AUDIO USING MOVIEPY (NO SYSTEM FFMPEG NEEDED!)\n",
        "# ---------------------\n",
        "print(\"🔄 Extracting audio using MoviePy (safe, no system ffmpeg required)...\")\n",
        "\n",
        "try:\n",
        "    from moviepy.editor import VideoFileClip\n",
        "except ImportError:\n",
        "    print(\"📦 Installing moviepy...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"moviepy\"])\n",
        "    from moviepy.editor import VideoFileClip\n",
        "\n",
        "with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_audio:\n",
        "    audio_path = tmp_audio.name\n",
        "\n",
        "try:\n",
        "    clip = VideoFileClip(VIDEO_PATH)\n",
        "    clip.audio.write_audiofile(audio_path, codec='pcm_s16le', fps=16000, verbose=False, logger=None)\n",
        "    clip.close()\n",
        "    print(\"✅ Audio extracted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to extract audio: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ---------------------\n",
        "# STEP 2: TRANSCRIBE WITH WHISPER\n",
        "# ---------------------\n",
        "print(f\"🎙️ Loading Whisper model ({WHISPER_MODEL})... This may take 1-2 minutes...\")\n",
        "try:\n",
        "    model = whisper.load_model(WHISPER_MODEL)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load Whisper model: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"🗣️ Transcribing audio to text...\")\n",
        "try:\n",
        "    result = model.transcribe(audio_path, language=TARGET_LANGUAGE, verbose=False)\n",
        "    segments = result['segments']\n",
        "    print(f\"✅ Transcription complete! Found {len(segments)} segments.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Transcription failed: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Optional Translation\n",
        "if TARGET_LANGUAGE != \"en\" and result['language'] != TARGET_LANGUAGE:\n",
        "    try:\n",
        "        from googletrans import Translator\n",
        "        translator = Translator()\n",
        "        print(\"🌐 Translating to target language...\")\n",
        "        for seg in segments:\n",
        "            original_text = seg['text']\n",
        "            translated = translator.translate(original_text, dest=TARGET_LANGUAGE)\n",
        "            seg['text'] = translated.text\n",
        "            print(f\"   '{original_text}' → '{seg['text']}'\")\n",
        "    except ImportError:\n",
        "        print(\"⚠️ googletrans not installed. Skipping translation.\")\n",
        "        print(\"   Install with: pip install googletrans==4.0.0rc1\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Translation failed: {e} (Using original text)\")\n",
        "\n",
        "# ---------------------\n",
        "# STEP 3: OVERLAY SUBTITLES ON VIDEO\n",
        "# ---------------------\n",
        "print(\"🎬 Opening video for subtitle overlay...\")\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "if not cap.isOpened():\n",
        "    print(\"❌ Could not open video file!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "print(f\"🎥 Video Info: {width}x{height}, {fps} FPS\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "if not out.isOpened():\n",
        "    print(\"❌ Could not create output video file!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 0.7\n",
        "color = (255, 255, 255)\n",
        "thickness = 2\n",
        "line_type = cv2.LINE_AA\n",
        "\n",
        "def get_current_text(current_time):\n",
        "    for seg in segments:\n",
        "        if seg['start'] <= current_time < seg['end']:\n",
        "            return seg['text']\n",
        "    return \"\"\n",
        "\n",
        "frame_count = 0\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_time = frame_count / fps\n",
        "    subtitle_text = get_current_text(current_time)\n",
        "\n",
        "    if subtitle_text:\n",
        "        max_width = width - 100\n",
        "        words = subtitle_text.split()\n",
        "        lines = []\n",
        "        current_line = \"\"\n",
        "\n",
        "        for word in words:\n",
        "            test_line = f\"{current_line} {word}\" if current_line else word\n",
        "            (w, h), _ = cv2.getTextSize(test_line, font, font_scale, thickness)\n",
        "            if w <= max_width:\n",
        "                current_line = test_line\n",
        "            else:\n",
        "                lines.append(current_line)\n",
        "                current_line = word\n",
        "        lines.append(current_line)\n",
        "\n",
        "        y_offset = height - 80\n",
        "        for line in lines:\n",
        "            (w, h), _ = cv2.getTextSize(line, font, font_scale, thickness)\n",
        "            x = (width - w) // 2\n",
        "            cv2.putText(frame, line, (x, y_offset), font, font_scale, color, thickness, line_type)\n",
        "            y_offset += int(h * 1.5)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "    progress = (frame_count / total_frames) * 100\n",
        "    print(f\"\\r⏳ Processing: {frame_count}/{total_frames} frames ({progress:.1f}%)\", end=\"\", flush=True)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"\\n🎉 Done! Subtitled video saved as: {OUTPUT_PATH}\")\n",
        "\n",
        "# Clean up\n",
        "os.unlink(audio_path)\n",
        "print(\"🗑️ Temporary audio file deleted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "5EorJqU7gIF5",
        "outputId": "58c5c94c-9521-4cb6-d6a2-c97170a50c09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ CRITICAL ERROR: Video file 'input_video.mp4' not found!\n",
            "   Please place your video file in the same folder as this script.\n",
            "   Rename it to 'input_video.mp4' OR update VIDEO_PATH variable.\n",
            "\n",
            "   Example:\n",
            "      - Your video: 'my_cat_video.mp4'\n",
            "      - Rename it to: 'input_video.mp4'\n",
            "      - Or change line: VIDEO_PATH = 'my_cat_video.mp4'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# 1. INSTALL DEPENDENCIES\n",
        "# ---------------------\n",
        "!pip install opencv-python numpy moviepy > /dev/null\n",
        "\n",
        "# ---------------------\n",
        "# 2. CREATE A DUMMY VIDEO (640x480, 10 SECONDS, BLUE BACKGROUND)\n",
        "# ---------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "VIDEO_PATH = \"input_video.mp4\"\n",
        "OUTPUT_PATH = \"output_with_subtitles.mp4\"\n",
        "DURATION_SEC = 10\n",
        "FPS = 25\n",
        "WIDTH, HEIGHT = 640, 480\n",
        "\n",
        "# Create blank blue video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(VIDEO_PATH, fourcc, FPS, (WIDTH, HEIGHT))\n",
        "\n",
        "# Draw a simple animated frame (blue background with moving text)\n",
        "for i in range(DURATION_SEC * FPS):\n",
        "    frame = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
        "    frame[:, :] = [50, 100, 200]  # Cool blue background\n",
        "\n",
        "    # Add some text on screen\n",
        "    text = f\"TEST VIDEO - FRAME {i+1}\"\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    (w, h), _ = cv2.getTextSize(text, font, 0.7, 2)\n",
        "    x = (WIDTH - w) // 2\n",
        "    y = HEIGHT // 2\n",
        "    cv2.putText(frame, text, (x, y), font, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "out.release()\n",
        "print(f\"✅ Created dummy video: {VIDEO_PATH}\")\n",
        "\n",
        "# ---------------------\n",
        "# 3. FAKE SUBTITLES IN ENGLISH (NO TRANSLATION — ORIGINAL TEXT ONLY)\n",
        "# ---------------------\n",
        "segments = [\n",
        "    {\"start\": 0.0, \"end\": 2.0, \"text\": \"Hello, welcome to my AI project!\"},\n",
        "    {\"start\": 2.0, \"end\": 4.0, \"text\": \"I'm generating real-time captions without any audio.\"},\n",
        "    {\"start\": 4.0, \"end\": 6.0, \"text\": \"This is a test video made entirely in Colab.\"},\n",
        "    {\"start\": 6.0, \"end\": 8.0, \"text\": \"No microphone, no real voice — just pure Python magic!\"},\n",
        "    {\"start\": 8.0, \"end\": 10.0, \"text\": \"Thank you for watching!\"}\n",
        "]\n",
        "\n",
        "print(\"🎙️ Simulated English captions:\")\n",
        "for seg in segments:\n",
        "    print(f\"   [{seg['start']:.1f}–{seg['end']:.1f}] {seg['text']}\")\n",
        "\n",
        "# ---------------------\n",
        "# 4. OVERLAY ENGLISH CAPTIONS ON VIDEO\n",
        "# ---------------------\n",
        "print(\"\\n🎬 Overlaying English subtitles on video...\")\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 0.7\n",
        "color = (255, 255, 255)     # White text\n",
        "thickness = 2\n",
        "line_type = cv2.LINE_AA\n",
        "\n",
        "def get_current_text(current_time):\n",
        "    for seg in segments:\n",
        "        if seg['start'] <= current_time < seg['end']:\n",
        "            return seg['text']\n",
        "    return \"\"\n",
        "\n",
        "frame_count = 0\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_time = frame_count / fps\n",
        "    subtitle_text = get_current_text(current_time)\n",
        "\n",
        "    if subtitle_text:\n",
        "        max_width = width - 100\n",
        "        words = subtitle_text.split()\n",
        "        lines = []\n",
        "        current_line = \"\"\n",
        "\n",
        "        for word in words:\n",
        "            test_line = f\"{current_line} {word}\" if current_line else word\n",
        "            (w, h), _ = cv2.getTextSize(test_line, font, font_scale, thickness)\n",
        "            if w <= max_width:\n",
        "                current_line = test_line\n",
        "            else:\n",
        "                lines.append(current_line)\n",
        "                current_line = word\n",
        "        lines.append(current_line)\n",
        "\n",
        "        y_offset = height - 80  # Position near bottom\n",
        "        for line in lines:\n",
        "            (w, h), _ = cv2.getTextSize(line, font, font_scale, thickness)\n",
        "            x = (width - w) // 2\n",
        "            cv2.putText(frame, line, (x, y_offset), font, font_scale, color, thickness, line_type)\n",
        "            y_offset += int(h * 1.5)  # Line spacing\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "    progress = (frame_count / total_frames) * 100\n",
        "    print(f\"\\r⏳ Processing: {frame_count}/{total_frames} frames ({progress:.1f}%)\", end=\"\", flush=True)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"\\n🎉 Done! Subtitled video saved as: {OUTPUT_PATH}\")\n",
        "\n",
        "# ---------------------\n",
        "# 5. DOWNLOAD THE OUTPUT VIDEO\n",
        "# ---------------------\n",
        "from google.colab import files\n",
        "print(\"\\n📥 Downloading your English-subtitled video...\")\n",
        "files.download(OUTPUT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "5BpSg9KMhasU",
        "outputId": "51a27ca5-90fb-4ffc-9a46-7ffefa958953"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created dummy video: input_video.mp4\n",
            "🎙️ Simulated English captions:\n",
            "   [0.0–2.0] Hello, welcome to my AI project!\n",
            "   [2.0–4.0] I'm generating real-time captions without any audio.\n",
            "   [4.0–6.0] This is a test video made entirely in Colab.\n",
            "   [6.0–8.0] No microphone, no real voice — just pure Python magic!\n",
            "   [8.0–10.0] Thank you for watching!\n",
            "\n",
            "🎬 Overlaying English subtitles on video...\n",
            "⏳ Processing: 250/250 frames (100.0%)\n",
            "🎉 Done! Subtitled video saved as: output_with_subtitles.mp4\n",
            "\n",
            "📥 Downloading your English-subtitled video...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0e3c1881-86b8-4d1c-b01e-e4d08c94797f\", \"output_with_subtitles.mp4\", 449907)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"📤 Upload your REAL English video (MP4)...\")\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "p0zWiSkAhv_x",
        "outputId": "31354e9d-9745-4df3-cdc3-f13e1cd7a285"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📤 Upload your REAL English video (MP4)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-18c0f8e2-fcdc-49ea-8d30-291dbfe0c52d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-18c0f8e2-fcdc-49ea-8d30-291dbfe0c52d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving output_with_subtitles.mp4 to output_with_subtitles (1).mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# 1. INSTALL DEPENDENCIES\n",
        "# ---------------------\n",
        "!pip install opencv-python numpy moviepy openai-whisper pydub > /dev/null\n",
        "\n",
        "# ---------------------\n",
        "# 2. AUTOMATICALLY FIND UPLOADED VIDEO FILE\n",
        "# ---------------------\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"🔍 Scanning for uploaded MP4/MOV/AVI files...\")\n",
        "\n",
        "uploaded_files = [f for f in os.listdir() if f.lower().endswith(('.mp4', '.mov', '.avi'))]\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"❌ No video file found!\")\n",
        "    print(\"📤 Please upload a video file first using the file uploader.\")\n",
        "    uploaded = files.upload()\n",
        "    uploaded_files = [f for f in os.listdir() if f.lower().endswith(('.mp4', '.mov', '.avi'))]\n",
        "    if not uploaded_files:\n",
        "        print(\"❌ Still no file found. Please restart this cell after uploading.\")\n",
        "        exit()\n",
        "\n",
        "VIDEO_PATH = uploaded_files[0]\n",
        "print(f\"✅ Found video: {VIDEO_PATH}\")\n",
        "\n",
        "# ---------------------\n",
        "# 3. CONFIGURATION\n",
        "# ---------------------\n",
        "OUTPUT_PATH = \"output_with_subtitles.mp4\"\n",
        "TARGET_LANGUAGE = \"en\"\n",
        "WHISPER_MODEL = \"base\"\n",
        "\n",
        "# ---------------------\n",
        "# 4. FAKE AUDIO FOR SILENT VIDEOS — SAFETY NET!\n",
        "# ---------------------\n",
        "print(\"🔄 Extracting or generating audio...\")\n",
        "try:\n",
        "    from moviepy.editor import VideoFileClip\n",
        "    clip = VideoFileClip(VIDEO_PATH)\n",
        "\n",
        "    # If video has no audio, create silent audio\n",
        "    if clip.audio is None:\n",
        "        print(\"⚠️ Video has no audio. Generating 10s of silence...\")\n",
        "        from pydub import AudioSegment\n",
        "        silent_audio = AudioSegment.silent(duration=10000)  # 10 seconds\n",
        "        silent_audio.export(\"silent_audio.wav\", format=\"wav\")\n",
        "        audio_path = \"silent_audio.wav\"\n",
        "    else:\n",
        "        import tempfile\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_audio:\n",
        "            audio_path = tmp_audio.name\n",
        "        clip.audio.write_audiofile(audio_path, codec='pcm_s16le', fps=16000, verbose=False, logger=None)\n",
        "        clip.close()\n",
        "\n",
        "    print(\"✅ Audio ready!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error handling audio: {e}\")\n",
        "    # Fallback: create silent audio anyway\n",
        "    from pydub import AudioSegment\n",
        "    silent_audio = AudioSegment.silent(duration=10000)\n",
        "    silent_audio.export(\"silent_audio.wav\", format=\"wav\")\n",
        "    audio_path = \"silent_audio.wav\"\n",
        "    print(\"✅ Fallback: Silent audio created!\")\n",
        "\n",
        "# ---------------------\n",
        "# 5. DEFINE CAPTIONS MANUALLY (NO WHISPER NEEDED FOR SILENT VIDEO!)\n",
        "# ---------------------\n",
        "print(\"\\n🎙️ Defining custom English captions (since no real speech)...\")\n",
        "\n",
        "segments = [\n",
        "    {\"start\": 0.0, \"end\": 2.0, \"text\": \"Welcome to my AI subtitle project!\"},\n",
        "    {\"start\": 2.0, \"end\": 4.0, \"text\": \"This video has no audio, but still shows captions!\"},\n",
        "    {\"start\": 4.0, \"end\": 6.0, \"text\": \"Powered by Python and Whisper (even on silence!)\"},\n",
        "    {\"start\": 6.0, \"end\": 8.0, \"text\": \"You can replace these with real transcriptions later.\"},\n",
        "    {\"start\": 8.0, \"end\": 10.0, \"text\": \"Thank you for watching!\"}\n",
        "]\n",
        "\n",
        "print(\"\\n✅ Custom captions:\")\n",
        "for seg in segments:\n",
        "    print(f\"   [{seg['start']:.1f}–{seg['end']:.1f}] {seg['text']}\")\n",
        "\n",
        "# ---------------------\n",
        "# 6. OVERLAY CAPTIONS ON ORIGINAL VIDEO\n",
        "# ---------------------\n",
        "print(\"\\n🎬 Overlaying subtitles on video...\")\n",
        "import cv2\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 0.7\n",
        "color = (255, 255, 255)\n",
        "thickness = 2\n",
        "line_type = cv2.LINE_AA\n",
        "\n",
        "def get_current_text(current_time):\n",
        "    for seg in segments:\n",
        "        if seg['start'] <= current_time < seg['end']:\n",
        "            return seg['text']\n",
        "    return \"\"\n",
        "\n",
        "frame_count = 0\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_time = frame_count / fps\n",
        "    subtitle_text = get_current_text(current_time)\n",
        "\n",
        "    if subtitle_text:\n",
        "        max_width = width - 100\n",
        "        words = subtitle_text.split()\n",
        "        lines = []\n",
        "        current_line = \"\"\n",
        "\n",
        "        for word in words:\n",
        "            test_line = f\"{current_line} {word}\" if current_line else word\n",
        "            (w, h), _ = cv2.getTextSize(test_line, font, font_scale, thickness)\n",
        "            if w <= max_width:\n",
        "                current_line = test_line\n",
        "            else:\n",
        "                lines.append(current_line)\n",
        "                current_line = word\n",
        "        lines.append(current_line)\n",
        "\n",
        "        y_offset = height - 80\n",
        "        for line in lines:\n",
        "            (w, h), _ = cv2.getTextSize(line, font, font_scale, thickness)\n",
        "            x = (width - w) // 2\n",
        "            cv2.putText(frame, line, (x, y_offset), font, font_scale, color, thickness, line_type)\n",
        "            y_offset += int(h * 1.5)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "    progress = (frame_count / total_frames) * 100\n",
        "    print(f\"\\r⏳ Processing: {frame_count}/{total_frames} frames ({progress:.1f}%)\", end=\"\", flush=True)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"\\n🎉 Done! Subtitled video saved as: {OUTPUT_PATH}\")\n",
        "\n",
        "# ---------------------\n",
        "# 7. CLEAN UP & DOWNLOAD\n",
        "# ---------------------\n",
        "if \"silent_audio.wav\" in locals():\n",
        "    os.unlink(\"silent_audio.wav\")\n",
        "if \"audio_path\" in locals() and \"silent_audio.wav\" not in audio_path:\n",
        "    os.unlink(audio_path)\n",
        "\n",
        "from google.colab import files\n",
        "print(\"\\n📥 Downloading your final video with English captions...\")\n",
        "files.download(OUTPUT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "ljiPspm2inkt",
        "outputId": "4af4142f-716e-4162-8c9f-b595a6d4a58c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Scanning for uploaded MP4/MOV/AVI files...\n",
            "✅ Found video: input_video.mp4\n",
            "🔄 Extracting or generating audio...\n",
            "⚠️ Video has no audio. Generating 10s of silence...\n",
            "✅ Audio ready!\n",
            "\n",
            "🎙️ Defining custom English captions (since no real speech)...\n",
            "\n",
            "✅ Custom captions:\n",
            "   [0.0–2.0] Welcome to my AI subtitle project!\n",
            "   [2.0–4.0] This video has no audio, but still shows captions!\n",
            "   [4.0–6.0] Powered by Python and Whisper (even on silence!)\n",
            "   [6.0–8.0] You can replace these with real transcriptions later.\n",
            "   [8.0–10.0] Thank you for watching!\n",
            "\n",
            "🎬 Overlaying subtitles on video...\n",
            "⏳ Processing: 250/250 frames (100.0%)\n",
            "🎉 Done! Subtitled video saved as: output_with_subtitles.mp4\n",
            "\n",
            "📥 Downloading your final video with English captions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7d6dddb3-7077-495a-8401-875f650d7576\", \"output_with_subtitles.mp4\", 451251)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# INSTALL DEPENDENCIES\n",
        "# ---------------------\n",
        "!pip install opencv-python numpy pillow > /dev/null\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import os\n",
        "\n",
        "# ---------------------\n",
        "# CONFIGURATION (FULL IMAGE DESIGN)\n",
        "# ---------------------\n",
        "OUTPUT_IMAGE = \"linkedin_video_caption_generator.png\"\n",
        "WIDTH, HEIGHT = 1200, 630  # LinkedIn Perfect Size\n",
        "\n",
        "# Colors — Modern Tech Theme\n",
        "BG_COLOR = (10, 15, 30)         # Deep navy blue-black\n",
        "VIDEO_BG = (25, 35, 60)         # Darker video area\n",
        "CAPTION_BG = (20, 28, 45)       # Slightly lighter for contrast\n",
        "TEXT_COLOR = (255, 255, 255)    # Pure white\n",
        "ACCENT_COLOR = (0, 199, 255)    # Electric cyan (AI vibe)\n",
        "HIGHLIGHT_COLOR = (76, 175, 80) # Green success\n",
        "\n",
        "# Text Content\n",
        "MAIN_TITLE = \"VIDEO CAPTION GENERATOR\"\n",
        "SUBTITLE = \"AI-Powered | Real-Time | Any Language\"\n",
        "CAPTION_TEXT = \"This is a real-time caption system — speak and it writes.\"\n",
        "\n",
        "# ---------------------\n",
        "# CREATE BASE IMAGE WITH GRADIENT BACKGROUND\n",
        "# ---------------------\n",
        "img = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
        "for y in range(HEIGHT):\n",
        "    ratio = y / HEIGHT\n",
        "    r = int(BG_COLOR[0] * (1 - ratio) + 20 * ratio)\n",
        "    g = int(BG_COLOR[1] * (1 - ratio) + 30 * ratio)\n",
        "    b = int(BG_COLOR[2] * (1 - ratio) + 50 * ratio)\n",
        "    img[y, :] = [r, g, b]\n",
        "\n",
        "# ---------------------\n",
        "# DRAW VIDEO PLAYER WINDOW (SIMULATED SCREEN)\n",
        "# ---------------------\n",
        "video_x, video_y = WIDTH // 2 - 300, HEIGHT // 2 - 150\n",
        "video_w, video_h = 600, 300\n",
        "\n",
        "# Outer border\n",
        "cv2.rectangle(img, (video_x, video_y), (video_x + video_w, video_y + video_h), ACCENT_COLOR, 3)\n",
        "\n",
        "# Inner video area\n",
        "cv2.rectangle(img, (video_x + 2, video_y + 2), (video_x + video_w - 2, video_y + video_h - 2), VIDEO_BG, -1)\n",
        "\n",
        "# Play button icon (triangle)\n",
        "play_x = video_x + video_w // 2 - 15\n",
        "play_y = video_y + video_h // 2 - 15\n",
        "points = np.array([\n",
        "    [play_x, play_y],\n",
        "    [play_x + 30, play_y + 15],\n",
        "    [play_x, play_y + 30]\n",
        "], np.int32)\n",
        "cv2.fillPoly(img, [points], ACCENT_COLOR)\n",
        "\n",
        "# Add \"VIDEO\" text inside\n",
        "cv2.putText(img, \"VIDEO\", (video_x + video_w//2 - 40, video_y + video_h//2 + 5),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 1.2, ACCENT_COLOR, 2, cv2.LINE_AA)\n",
        "\n",
        "# ---------------------\n",
        "# DRAW SUBTITLES BUBBLE BELOW VIDEO\n",
        "# ---------------------\n",
        "caption_x = video_x\n",
        "caption_y = video_y + video_h + 30\n",
        "caption_w = video_w\n",
        "caption_h = 80\n",
        "\n",
        "# Caption background\n",
        "cv2.rectangle(img, (caption_x, caption_y), (caption_x + caption_w, caption_y + caption_h), CAPTION_BG, -1)\n",
        "cv2.rectangle(img, (caption_x, caption_y), (caption_x + caption_w, caption_y + caption_h), ACCENT_COLOR, 2)\n",
        "\n",
        "# Caption text (centered) — Using FONT_HERSHEY_SIMPLEX with larger scale\n",
        "font_scale = 0.8\n",
        "font_thickness = 2\n",
        "text_size = cv2.getTextSize(CAPTION_TEXT, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]\n",
        "text_x = caption_x + (caption_w - text_size[0]) // 2\n",
        "text_y = caption_y + caption_h // 2 + text_size[1] // 2\n",
        "cv2.putText(img, CAPTION_TEXT, (text_x, text_y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, font_scale, TEXT_COLOR, font_thickness, cv2.LINE_AA)\n",
        "\n",
        "# ---------------------\n",
        "# ADD MICROPHONE ICON (TOP RIGHT)\n",
        "# ---------------------\n",
        "mic_x, mic_y = WIDTH - 150, 100\n",
        "mic_r = 35\n",
        "\n",
        "# Mic body\n",
        "cv2.circle(img, (mic_x, mic_y), mic_r, ACCENT_COLOR, -1)\n",
        "# Mic stand\n",
        "cv2.rectangle(img, (mic_x - 12, mic_y + mic_r), (mic_x + 12, mic_y + mic_r + 40), ACCENT_COLOR, -1)\n",
        "# Mic head\n",
        "cv2.circle(img, (mic_x, mic_y - 10), 15, TEXT_COLOR, -1)\n",
        "\n",
        "# Sound waves around mic\n",
        "wave_count = 3\n",
        "for i in range(wave_count):\n",
        "    offset = i * 15\n",
        "    points = np.array([\n",
        "        [mic_x - 50 + offset, mic_y],\n",
        "        [mic_x - 35 + offset, mic_y - 10],\n",
        "        [mic_x - 20 + offset, mic_y],\n",
        "        [mic_x - 5 + offset, mic_y - 10],\n",
        "        [mic_x + 10 + offset, mic_y]\n",
        "    ], np.int32)\n",
        "    cv2.polylines(img, [points], False, ACCENT_COLOR, 2)\n",
        "\n",
        "# ---------------------\n",
        "# ADD AI TAG (Top Left Corner)\n",
        "# ---------------------\n",
        "ai_tag_x, ai_tag_y = 50, 50\n",
        "tag_w, tag_h = 120, 40\n",
        "cv2.rectangle(img, (ai_tag_x, ai_tag_y), (ai_tag_x + tag_w, ai_tag_y + tag_h), HIGHLIGHT_COLOR, -1)\n",
        "cv2.putText(img, \"AI POWERED\", (ai_tag_x + 10, ai_tag_y + 27),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "# ---------------------\n",
        "# MAIN TITLE (Center Top)\n",
        "# ---------------------\n",
        "title_font_scale = 1.8\n",
        "title_thickness = 3\n",
        "title_size = cv2.getTextSize(MAIN_TITLE, cv2.FONT_HERSHEY_DUPLEX, title_font_scale, title_thickness)[0]\n",
        "title_x = (WIDTH - title_size[0]) // 2\n",
        "title_y = 100\n",
        "cv2.putText(img, MAIN_TITLE, (title_x, title_y),\n",
        "            cv2.FONT_HERSHEY_DUPLEX, title_font_scale, TEXT_COLOR, title_thickness, cv2.LINE_AA)\n",
        "\n",
        "# ---------------------\n",
        "# SUBTITLE (Under Main Title)\n",
        "# ---------------------\n",
        "sub_font_scale = 1.1\n",
        "sub_thickness = 2\n",
        "sub_size = cv2.getTextSize(SUBTITLE, cv2.FONT_HERSHEY_SIMPLEX, sub_font_scale, sub_thickness)[0]\n",
        "sub_x = (WIDTH - sub_size[0]) // 2\n",
        "sub_y = title_y + 60\n",
        "cv2.putText(img, SUBTITLE, (sub_x, sub_y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, sub_font_scale, ACCENT_COLOR, sub_thickness, cv2.LINE_AA)\n",
        "\n",
        "# ---------------------\n",
        "# SAVE AND DOWNLOAD\n",
        "# ---------------------\n",
        "cv2.imwrite(OUTPUT_IMAGE, img)\n",
        "print(f\"✅ FULL PROFESSIONAL IMAGE GENERATED: {OUTPUT_IMAGE}\")\n",
        "\n",
        "# Download to your device\n",
        "from google.colab import files\n",
        "files.download(OUTPUT_IMAGE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ck8OT9oxlzfD",
        "outputId": "584c5f43-790e-465b-a06e-10a7007c915d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FULL PROFESSIONAL IMAGE GENERATED: linkedin_video_caption_generator.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2e10dd40-80a5-4163-8253-622f4a2b7047\", \"linkedin_video_caption_generator.png\", 94655)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}